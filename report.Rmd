---
title: "French Presidents Investiture Speech Analysis"
author: "Lucas Guichard (50221623)"
date: "2023, Thursday 15th June"
output:
    html_document:
        code_folding: hide
        fig_caption: yes
---

J'ai choisi de réaliser ce projet final sur la politique française. Pour cela, j'ai pu collecté d'un site officiel du gouvernenement, le discours d'investiture de chaque président français de la cinquième république.
L'objectif de cette étude est donc de voir une évolution dans leur discours cohérente avec l'histoire de la France.

<br />

Lien vers github : https://github.com/kounen/FrenchPresidentsInvestitureSpeech

<br />

```{r include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # To avoid warning in HTML page
```

# Install the necessary packages
```{r}
# Uncomment to install
# install.packages("dplyr")
# install.packages("tidytext")
# install.packages("stopwords")
# install.packages("SnowballC")
# install.packages("ggraph")
# install.packages("tidyr")
# install.packages("tidygraph")
# install.packages("widyr")
```

# Load the necessary packages
```{r}
library(dplyr) # pipe operator
library(tidytext) # unnest_tokens
library(stopwords) # to remove french and english stop words
library(SnowballC) # for stemming
library(ggraph) # for visual graph construction
library(tidyr) # for pivot_wider() function
library(tidygraph) # for data graph construction
library(widyr) # for pairwise_cor()
```

# Load the dataset
J'ai du recréer moi-même un csv à partir des discours trouvés. Pour chaque, j'ai précisé la date d'investiture, le président concerné ainsi que le lien vers la source d'origine.
A prendre en compte que le texte original est français et que j'ai donc dû le traduire. Il faut donc prendre en compte lors de cette analyse, qu'une traduction étant une interprétation, les résultats ne peuvent être considéré comme véridique à 100%.
```{r}
speeches_data <- read.csv('data/Speeches.csv', sep=';') 
speeches_data
```

<br />

# Compare speeches' length
Avant de passer à tout traitement de pré-processing, j'ai souhaité comparer la longueur de chaque.
On peut ainsi très clairement voir que la longueur des discours n'a fait qu'augmenter depuis le début de cette cinquième république. Mais pourquoi? Est-ce parce qu'à l'époque, les présidents ne resentaient pas le besoin d'étaler leur futur mandat ou bien juste car aujourd'hui, même élu, un discours d'investiture est si important pour le président et surtout politisable qu'il doit, 'pour les caméras', prendre soin d'annoncer, remercier...
```{r}
# Add a column containing for each speech, its length
speeches_length <- speeches_data %>%
  mutate(Length = nchar(Speech))

# Historical order
speeches_length$President <- factor(speeches_length$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create a speech length bar graph to compare them
ggplot(speeches_length, aes(x = President, y = Length)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "President", y = "Speech Length") +
  ggtitle("Inauguration Speech Length Comparison") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.x = element_blank(), # Remove x-axis grid lines
        axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for better readability
```

<br />

# Tokenize the text into words, remove stop words (french and english) and stem the surviving ones
Ici, nous allons supprimer les mots "stop" à la fois anglais mais aussi français dans le doute qu'il en reste.
Vous pouvez aussi si vous le souhaitez décommenté la ligne 'stemming' pour obtenir une analyse qui n'est pas sujette à des perturbations telles que singulier/pluriel, verbe infinitif/conjugué...
```{r}
english_stopwords <- stopwords("en")
french_stopwords <- stopwords("fr")

speeches_tidy <- speeches_data %>%
  # Group by President
  group_by(President) %>%
  # Tokenize
  unnest_tokens(output = word, input = Speech) %>%
  # Added line number for future analysis (one line contains 10 words)
  mutate(line_number = ceiling(row_number() / 10)) %>%
  # Remove stop words
  anti_join(data.frame(word = english_stopwords), by = "word") %>%
  anti_join(data.frame(word = french_stopwords), by = "word") # Uncoment if you want stemming %>%
  # Apply stemming
  # Uncoment if you want stemming mutate(word = wordStem(word, language = "en"))
speeches_tidy
```

<br />

# Analysis - TF (Term Frequency)
On peut remarque que les termes désignant la France et les Français sont nombreux. Mais aussi l'internationalisation du discours. Chez Mr Mitterant, les mots Paris et city en tête de liste, démontre une vision plus accès sur l'échelle nationale. A l'opposé des discours arrivant dès 2002 (développement de l'Europe...) où l'on peut voir les mots "world" chez les Présidents Chirac, Hollande et Macron.
```{r}
# Extract the top 10 words in terms of frequency from each president's speeches
top10tf <- speeches_tidy %>%
  count(President, word) %>%
  group_by(President) %>%
  slice_max(n, n = 10, with_ties = F) # False to avoid multiple elements with the same value

# Historical order
top10tf$President <- factor(top10tf$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Define my color palette
my_colors <- c("blue", "red", "red", "orange", "orange", "green", "magenta", "brown", "brown")

# Create the top 10 tf bar graph
ggplot(top10tf, aes(x = reorder_within(word, n, President), # To order by word frequency
                  y = n,
                  fill = President)) +
  geom_col(show.legend = F) + # To remove President legend
  coord_flip() + # To flip axis
  facet_wrap(~President, scales = "free", ncol = 2) + # To remove unused top10 words + space on x axis
  scale_x_reordered() + # To remove x axis legends
  scale_fill_manual(values = my_colors) + # To use my defined colors
  labs(x = NULL) + # To remove reorder labels
  ggtitle("Top 10 words by Term Frequency") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) # Remove y-axis grid lines
  
```
<center>
<img src="images/tf.png" width=1000>
</center>

<br />

# Analysis - TF-IDF (Term Frequency - Inverse Document Frequency)
Formula used to compute tf-idf.
$${\large\text{TF-IDF} = TF{\times}\log\frac{{\text{N}}}{{\text{DF}}}}$$
Le graphe TF-IDF nous permet de voir davantage de mots différents d'un discours à l'autre.
En effet, la présence de termes concernant le climat est assez hétérogène. Absente jusqu'en 2017 des discours, cette mention concorde avec les luttes sociales sur l'environnement qui sont porté depuis les 10 dernières années en Europe et en France (Greta Thunberg).
Le premier discours appel à une nouvelle 'era' alors que les suivants vont plutôt faire éloge de leurs prédécesseurs avec 'decades', 'achieved', 'done'... De plus, ils pensent presque tous au travail qu'il y a maintenant à accomplir sur leur nouveau mandat : 'need', 'demand', 'growth'...
Le discours de François Hollande souligne bien la crise économique connue dans la fin des années 2010. Les termes 'whatever', 'indispensable', 'hard', 'growth' et 'debt' underline this.
```{r}
# - An indicator of the degree to which a word is uncommon but frequently used in a specific text
# - Used to find key words that reveal the personality of a text

# Extract the top 10 words in terms of tf-idf from each president's speeches
top10tfidf <- speeches_tidy %>%
  count(President, word) %>%
  # Combine the term frequency (TF) and inverse document frequency (IDF) calculations into a single step
  bind_tf_idf(term = word,           # Word
              document = President,  # Text delimiter
              n = n) %>%             # Word frequency
  group_by(President) %>%
  slice_max(tf_idf, n = 10, with_ties = F) # False to avoid multiple elements with the same value

# Historical order
top10tfidf$President <- factor(top10tfidf$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create the top 10 tf-idf bar graph
ggplot(top10tfidf, aes(x = reorder_within(word, tf_idf, President), # To order by tf-idf
                  y = tf_idf,
                  fill = President)) +
  geom_col(show.legend = F) + # To remove President legend
  coord_flip() + # To flip axis
  facet_wrap(~President, scales = "free", ncol = 2) + # To remove unused top10 words + space on x axis
  scale_x_reordered() + # To remove x axis legends
  scale_fill_manual(values = my_colors) + # To use my previous defined colors
  labs(x = NULL) + # To remove reorder labels
  ggtitle("Top 10 words by Term Frequency - Inverse Document Frequency") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) # Remove y-axis grid lines
```
<center>
<img src="images/tfidf.png" width=1000>
</center>

<br />

# Sentimental Analysis using 'bing' lexicon
Goal: Compare for each speech the distribution of positive and negative words.
Then, to compare only the positive and negative aspects, let's use "bing", which is the most inclusive lexicon for this.
On observe une bonne balance dans la répartition de mots positifs et négatifs dans chaque discours. Environ 75% de positifs pour 25% de mots négatifs. C'est un bon ratio sachant qu'un discours d'investiture se doit de motiver mais aussi rappeler les défis à surmonter dans les prochaines années.
```{r}
speeches_posneg <- speeches_tidy %>%
  inner_join(get_sentiments("bing")) %>% # Use 'bing' lexicon
  count(President, sentiment) %>%
  group_by(President) %>%
  mutate(percentage = n / sum(n) * 100) %>% # Compute distribution in %
  rename(Sentiment = sentiment) # Rename sentiment column for aesthetics reason (legend displaying)

# Historical order
speeches_posneg$President <- factor(speeches_posneg$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create the circular graphs
ggplot(speeches_posneg, aes(x = "", y = percentage, fill = Sentiment)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  geom_text(aes(
    label = paste0(round(percentage), "%")), # Display percentage values on each graph
    position = position_stack(vjust = 0.5),
    color = "white",
    size = 3) +
  coord_polar("y", start = 0) +
  facet_wrap(~President, strip.position = "bottom", ncol = 3, labeller = label_wrap_gen(width = 20)) +
  scale_fill_manual(values = c("negative" = "red", "positive" = "darkgreen")) +
  theme_void() + # Hide ugly graph details
  ggtitle("Speeches positive/negative words distribution") +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold", hjust = 0.5),
    strip.text = element_text(size = 8, face = "bold"),
    legend.margin = margin(t = 10, r = 10, b = 10, l = 50)
  )
```

<br />

# Sentimental Analysis using 'nrc' lexicon
Problème avec la précédente analyse. Nous n'avons pas assez de substances pour faire une véritable analyse sentimentale.
Mais ce n'est pas compter sur le lexicon 'nrc'.
Most of the time, we use 'nrc' lexicon only for pure sentiment analysis (positive or negative).
Nonetheless, this lexicon contains also 8 emotions.
Let's use them to implement a more exhaustive analysis.
```{r}
# Define a chunk (group of lines)
chunk_size <- 20 # lines

# Get sentiment dataframe
speeches_sentiment <- speeches_tidy %>%
  inner_join(get_sentiments("nrc")) %>% # Use 'nrc' lexicon
  count(President, index = line_number %/% chunk_size, sentiment) %>% # Create chunk
  group_by(President, index) %>%
  mutate(total_words = sum(n)) %>%
  ungroup() %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% # Reshape table
  mutate_at(vars(anger, anticipation, disgust, fear, joy, sadness, surprise, trust), ~ . / total_words * 100) # Calculate a percentage for each sentiment

# Historical order
speeches_sentiment$President <- factor(speeches_sentiment$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Define a custom color palette for each sentiment
sentiment_palette <- c("anger" = "red", "anticipation" = "lightgreen", "disgust" = "pink",
                       "fear" = "darkgray", "joy" = "yellow", "negative" = "darkred",
                       "positive" = "darkgreen", "sadness" = "purple", "surprise" = "orange",
                       "trust" = "blue")

# Reshape the data into long format
speeches_sentiment <- speeches_sentiment %>%
  tidyr::pivot_longer(cols = c(anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, trust),
                      names_to = "Sentiment",
                      values_to = "Percentage")

# Create the stacked bar graph
ggplot(speeches_sentiment, aes(x = factor(index), y = Percentage, fill = Sentiment)) +
  geom_bar(stat = "identity", position = "stack") +
  facet_wrap(~President, ncol = 2, scales = "free_x") +
  labs(x = paste0("Chunk (", chunk_size, " lines)"), y = "Percentage") + # Dynamic legend
  ggtitle("Sentiment Analysis by Chunk and President") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) + # Remove y-axis grid lines
  scale_fill_manual(values = sentiment_palette) # Use our custom color palette
```

On peut très facilement voir que la majorité des Présidents cherche un sentiment de confiance chez leur auditoire.
De plus, le sentiment de joie est très clairement présent. Ce qui n'est pas étonnant après des mois de campagne présidentiel mais aussi obtenir la fonction la plus haute d'un pays.
On peut également voir davantage d'émotions négatives comme la peur, la colère et le dégoût pour les années 2007 et 2017. Cela est cohérent avec l'histoire où respectivement une importante crise économique et une hausse du nombres d'attentats en France se sont manifestées.
<center>
<img src="images/emotions.png" width=1000>
</center>

<br />

# Log Odds Ratio Analysis for Macron's speeches
As you know, President Macron is the actual French President but was already President before.
Let's compare his two investiture speeches to find some differences.
For this purpose, what's better than using log odds ration method.
$${\large\text{log odds ratio} = \log{\left(\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}}
                              {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}\right)}}$$
Cette analyse des discours du Président Macron est assez intéressante. Premièrement, c'est elle qui pourrait être la plus véridique de part la taille de ces discours qui sont les plus importants. On peut donc voir que sur ces deux mandats, le Président a évolué sur sa position. Je pense par exemple au climat qui venait en dernier sur son premier mandat alors qu'il y apporte suffisament d'importance lrs de son second mandat pour que notre analyse place la cause écologique en quatrième position : 'ecological'.
Doit aussi être évoqué que lors de son premier mandat, il va chercher à parler des dernières décénies, alors que pour son second discours 5 ans plutard, il va souligné l'importance de la "continuité" de "projet". Ainsi que de la nouveauté qu'il peut toujours amméner à la France ('new').
```{r}
# Extract the top 10 words in terms of log odds ratio from Macron's speeches (2017 and 2022)
top10lor <- speeches_tidy %>%
  filter(President %in% c("Emmanuel Macron (2017)", "Emmanuel Macron (2022)")) %>%
  count(President, word) %>%
  pivot_wider(names_from = President,
              values_from = n,
              values_fill = list(n = 0)) %>%
  rename(EM2017 = `Emmanuel Macron (2017)`,
         EM2022 = `Emmanuel Macron (2022)`) %>%
  # Add `+1` to all values so that the frequency is greater than zero
  mutate(ratio_EM2017 = ((EM2017 + 1) / (sum(EM2017 + 1))), # Weight of words in EM2017 speech
         ratio_EM2022 = ((EM2022 + 1) / (sum(EM2022 + 1))), # Weight of words in EM2022 speech
         odds_ratio = ratio_EM2017 / ratio_EM2022,
         log_odds_ratio = log(odds_ratio)) %>%
  group_by(President = ifelse(log_odds_ratio > 0, "EM2017", "EM2022")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F) %>%
  select(word, log_odds_ratio, President)

# Refill president column with President's full name
top10lor$President[top10lor$President == "EM2017"] <- "Emmanuel Macron (2017)"
top10lor$President[top10lor$President == "EM2022"] <- "Emmanuel Macron (2022)"

# Historical order
top10lor$President <- factor(top10lor$President,
                             levels = c("Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create the top 10 log odds ratio bar graph
ggplot(top10lor, aes(x = reorder_within(word, log_odds_ratio, President), # To order by log odds ratio
                  y = log_odds_ratio,
                  fill = President)) +
  geom_col(show.legend = F) + # To remove President legend
  coord_flip() + # To flip axis
  facet_wrap(~President, scales = "free", ncol = 2) + # To remove unused top10 words + space on x axis
  scale_x_reordered() + # To remove x axis legends
  labs(x = NULL) + # To remove reorder labels
  ggtitle("Top 10 words by Log Odds Ration for Macron's speeches") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) # Remove y-axis grid lines
```

- The *sign** and **magnitude** tell us which words are more important in the speech
- A positive number greater than 0 indicates greater importance in E.M. 2017's speech
- Negative numbers less than 0 have more weight in E.M. 2022's speech
- If it is close to 0, it has similar weight in both speeches
<br />
<br />

# Speeches' Clusters by analyzing consecutive word pairs with n-gram
Pour finir, nous allons essayer de capturer une vue plus globale de l'ensemble des discours.
On peut voir les noms de certains présidents ressortir et plus précisément les premiers. On peut donc penser que nos derniers présidents ont eu à coeur de les remercier lors de leur discour.
Enfin certaines expression sont présentes. Comme 'vive la république' qui vient clôturer presque tout discours présidentiel.
L'importance des citoyens dans une république est démontré avec le clusteur rouge du bas qui est le plus important de ce réseau.
Enfin remarques intéressantes, la célèbre citation française 'Liberté, Egalité, Fraternité' est présente dans ce réseau mais sans le mot 'Fraternité'. Pourquoi? Est-ce car le candidat élu ne souhaite pas considéré l'ensemble des citoyens comme des frères car ils n'ont pas tous votés pour lui?

By making a network graph with Bigrams, we can:

  - Focus on pairs of words that occur frequently together in succession
  - Represented by highly related and simultaneously frequent words
  - Nodes are mostly connected
  - Clustering of words is less clear, but overall relationship of words can be seen
```{r}
# Creating our Bigrams data frame
bigram_speeches <- speeches_data %>%
  unnest_tokens(input = Speech,
                output = bigram,
                token = "ngrams",
                n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  # Remove stop words
  filter(!word1 %in% c(english_stopwords, french_stopwords),
         !word2 %in% c(english_stopwords, french_stopwords)) %>%
  count(word1, word2, sort = T) %>%
  # Remove NA values after stop words cleaning
  na.omit()

# Creating a network graph data
graph_bigram <- bigram_speeches %>%
  # Flimsy filter (we don't have a lot of data here)
  filter(n >= 3) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(), # Compute centrality
         group = as.factor(group_infomap())) # Compute community

# Creating our Bigrams network graph
set.seed(1234)
ggraph(graph_bigram, layout = "fr") +
  geom_edge_link(color = "gray50",       # Edge color
                 alpha = 0.5) +          # Edge contrast
  geom_node_point(aes(size = centrality, # Node size
                      color = group),    # Node color
                  show.legend = F) +     # Legend removal
  scale_size(range = c(5, 10)) +         # Range of node size
  geom_node_text(aes(label = name),
                 repel = T,
                 size = 5) +
  theme_graph() +
  ggtitle("Speeches Bigrams network graph") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) # Bold title, centered
```

N-grams:

- Center around pairs of words that make sense when used together in succession
- When expressing the overall relationship between words  
<br />

# Speeches' Clusters with Phi Coefficient
Afin d'avoir un point de vue sur les paires plus uniques dans ces discours, utilisons le Phi coefficient.
On se rend compte ici qu'un cluster ressort très clairement de la généralité. Celui en rose, contenant tout ce qui concerne Paris, son maire et la ville en général. On comprend très clairement qu'au début de cette nouvelle période qu'est la cinquième république, Paris c'était la France et que l'attention politique y était centré. Pensée qui a heureusement évoluée et s'est globalisée comme le montre l'ensemble de nos analyses.
By making a network graph with Phi Coefficient, we can:

- Network around highly relevant word pairs
- Highly relevant words are represented even if they are less frequent
- Less relevant nodes are not connected
- Clusters of words are clearly visible, but it is difficult to see the overall relationship between words
```{r}
# Creating our Phi Coefficient data frame
phi_speeches <- speeches_tidy %>%
  add_count(word) %>%
  # Flimsy filter (we don't have a lot of data here)
  filter(n >= 5) %>%
  pairwise_cor(item = word, # Compute Phi Coefficient
               feature = President, 
               sort = T)

# Creating a network graph data
graph_phi <- phi_speeches %>%
  # Correlation must be equal or bigger than 0.05
  filter(correlation >= 0.05) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(),
         group = as.factor(group_infomap()))

# Creating our Bigrams network graph
set.seed(1234)
ggraph(graph_phi, layout = "fr") +
  geom_edge_link(color = "gray50",
                 aes(edge_alpha = correlation, # Edge contrast
                     edge_width = 1),          # Edge thickness
                 show.legend = F) +            # Legend removal
  scale_edge_width(range = c(1, 2)) +          # Edge thickness removal
  geom_node_point(aes(size = centrality,
                      color = group),
                  show.legend = F) +
  scale_size(range = c(5, 10)) +
  geom_node_text(aes(label = name),
                 repel = T,
                 size = 5) +
  theme_graph() +
  ggtitle("Speeches Phi Coefficient network graph") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) # Bold title, centered
```

Phi coefficient:

- Focus on highly related word pairs
- When you want to highlight clusters of words

# Conclusion

On voit ainsi dans cette analyse, que la France en tant que puissance mondiale, n'a pas toujours eu cette envie politique de monter autant sur la scène internationale. Sûrement après les dégâts que le pays à pu connaître suite aux Guerres mondiales.
De plus certains éléments marquants (terrorisme, crise économique) sont soulignés dans les résultats obtenus. Montrant donc l'importance historique de tels événements.