---
title: "French Presidents Investiture Speech Analysis"
author: "Lucas Guichard (50221623)"
date: "2023, Thursday 15th June"
output:
    html_document:
        code_folding: hide
        fig_caption: yes
---

<br />

Lien vers github : https://github.com/kounen/FrenchPresidentsInvestitureSpeech

<br />

```{r include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # To avoid warning in HTML page
```

# Install the necessary packages
```{r}
# Uncomment to install
# install.packages("dplyr")
# install.packages("tidytext")
# install.packages("stopwords")
# install.packages("SnowballC")
# install.packages("ggraph")
# install.packages("tidyr")
# install.packages("tidygraph")
# install.packages("widyr")
```

# Load the necessary packages
```{r}
library(dplyr) # pipe operator
library(tidytext) # unnest_tokens
library(stopwords) # to remove french and english stop words
library(SnowballC) # for stemming
library(ggraph) # for visual graph construction
library(tidyr) # for pivot_wider() function
library(tidygraph) # for data graph construction
library(widyr) # for pairwise_cor()
```

# Load the dataset
```{r}
speeches_data <- read.csv('data/Speeches.csv', sep=';') 
speeches_data
```

<br />

# Compare speeches' length
```{r}
# Add a column containing for each speech, its length
speeches_length <- speeches_data %>%
  mutate(Length = nchar(Speech))

# Historical order
speeches_length$President <- factor(speeches_length$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create a speech length bar graph to compare them
ggplot(speeches_length, aes(x = President, y = Length)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "President", y = "Speech Length") +
  ggtitle("Inauguration Speech Length Comparison") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.x = element_blank(), # Remove x-axis grid lines
        axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels for better readability
```

<br />

# Tokenize the text into words, remove stop words (french and english) and stem the surviving ones
```{r}
english_stopwords <- stopwords("en")
french_stopwords <- stopwords("fr")

speeches_tidy <- speeches_data %>%
  # Group by President
  group_by(President) %>%
  # Tokenize
  unnest_tokens(output = word, input = Speech) %>%
  # Added line number for future analysis (one line contains 10 words)
  mutate(line_number = ceiling(row_number() / 10)) %>%
  # Remove stop words
  anti_join(data.frame(word = english_stopwords), by = "word") %>%
  anti_join(data.frame(word = french_stopwords), by = "word") # Uncoment if you want stemming %>%
  # Apply stemming
  # Uncoment if you want stemming mutate(word = wordStem(word, language = "en"))
speeches_tidy
```

<br />

# Analysis - TF (Term Frequency)
```{r}
# Extract the top 10 words in terms of frequency from each president's speeches
top10tf <- speeches_tidy %>%
  count(President, word) %>%
  group_by(President) %>%
  slice_max(n, n = 10, with_ties = F) # False to avoid multiple elements with the same value

# Historical order
top10tf$President <- factor(top10tf$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Define my color palette
my_colors <- c("blue", "red", "red", "orange", "orange", "green", "magenta", "brown", "brown")

# Create the top 10 tf bar graph
ggplot(top10tf, aes(x = reorder_within(word, n, President), # To order by word frequency
                  y = n,
                  fill = President)) +
  geom_col(show.legend = F) + # To remove President legend
  coord_flip() + # To flip axis
  facet_wrap(~President, scales = "free", ncol = 2) + # To remove unused top10 words + space on x axis
  scale_x_reordered() + # To remove x axis legends
  scale_fill_manual(values = my_colors) + # To use my defined colors
  labs(x = NULL) + # To remove reorder labels
  ggtitle("Top 10 words by Term Frequency") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) # Remove y-axis grid lines
  
```
<center>
<img src="images/tf.png" width=1000>
</center>

<br />

# Analysis - TF-IDF (Term Frequency - Inverse Document Frequency)
$${\large\text{TF-IDF} = TF{\times}\log\frac{{\text{N}}}{{\text{DF}}}}$$
```{r}
# - An indicator of the degree to which a word is uncommon but frequently used in a specific text
# - Used to find key words that reveal the personality of a text

# Extract the top 10 words in terms of tf-idf from each president's speeches
top10tfidf <- speeches_tidy %>%
  count(President, word) %>%
  # Combine the term frequency (TF) and inverse document frequency (IDF) calculations into a single step
  bind_tf_idf(term = word,           # Word
              document = President,  # Text delimiter
              n = n) %>%             # Word frequency
  group_by(President) %>%
  slice_max(tf_idf, n = 10, with_ties = F) # False to avoid multiple elements with the same value

# Historical order
top10tfidf$President <- factor(top10tfidf$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create the top 10 tf-idf bar graph
ggplot(top10tfidf, aes(x = reorder_within(word, tf_idf, President), # To order by tf-idf
                  y = tf_idf,
                  fill = President)) +
  geom_col(show.legend = F) + # To remove President legend
  coord_flip() + # To flip axis
  facet_wrap(~President, scales = "free", ncol = 2) + # To remove unused top10 words + space on x axis
  scale_x_reordered() + # To remove x axis legends
  scale_fill_manual(values = my_colors) + # To use my previous defined colors
  labs(x = NULL) + # To remove reorder labels
  ggtitle("Top 10 words by Term Frequency - Inverse Document Frequency") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) # Remove y-axis grid lines
```
<center>
<img src="images/tfidf.png" width=1000>
</center>

<br />

# Sentimental Analysis using 'nrc' lexicon
Most of the time, we use 'nrc' lexicon only for pure sentiment analysis (positive or negative).
Nonetheless, this lexicon contains also 8 emotions.
Let's use them to implement a more exhaustive analysis.
```{r}
# Define a chunk (group of lines)
chunk_size <- 20 # lines

# Get sentiment dataframe
speeches_sentiment <- speeches_tidy %>%
  inner_join(get_sentiments("nrc")) %>% # Use 'nrc' lexicon
  count(President, index = line_number %/% chunk_size, sentiment) %>% # Create chunk
  group_by(President, index) %>%
  mutate(total_words = sum(n)) %>%
  ungroup() %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% # Reshape table
  mutate_at(vars(anger, anticipation, disgust, fear, joy, sadness, surprise, trust), ~ . / total_words * 100) # Calculate a percentage for each sentiment

# Historical order
speeches_sentiment$President <- factor(speeches_sentiment$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Define a custom color palette for each sentiment
sentiment_palette <- c("anger" = "red", "anticipation" = "lightgreen", "disgust" = "pink",
                       "fear" = "darkgray", "joy" = "yellow", "negative" = "darkred",
                       "positive" = "darkgreen", "sadness" = "purple", "surprise" = "orange",
                       "trust" = "blue")

# Reshape the data into long format
speeches_sentiment <- speeches_sentiment %>%
  tidyr::pivot_longer(cols = c(anger, anticipation, disgust, fear, joy, negative, positive, sadness, surprise, trust),
                      names_to = "Sentiment",
                      values_to = "Percentage")

# Create the stacked bar graph
ggplot(speeches_sentiment, aes(x = factor(index), y = Percentage, fill = Sentiment)) +
  geom_bar(stat = "identity", position = "stack") +
  facet_wrap(~President, ncol = 2, scales = "free_x") +
  labs(x = paste0("Chunk (", chunk_size, " lines)"), y = "Percentage") + # Dynamic legend
  ggtitle("Sentiment Analysis by Chunk and President") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) + # Remove y-axis grid lines
  scale_fill_manual(values = sentiment_palette) # Use our custom color palette
```
<center>
<img src="images/emotions.png" width=1000>
</center>

<br />

# Log Odds Ratio Analysis for Macron's speeches
As you know, President Macron is the actual French President but was already President before.
Let's compare his two investiture speeches to find some differences.
For this purpose, what's better than using log odds ration method.
$${\large\text{log odds ratio} = \log{\left(\frac{\left(\frac{n+1}{\text{total}+1}\right)_\text{Text A}}
                              {\left(\frac{n+1}{\text{total}+1}\right)_\text{Text B}}\right)}}$$
```{r}
# Extract the top 10 words in terms of log odds ratio from Macron's speeches (2017 and 2022)
top10lor <- speeches_tidy %>%
  filter(President %in% c("Emmanuel Macron (2017)", "Emmanuel Macron (2022)")) %>%
  count(President, word) %>%
  pivot_wider(names_from = President,
              values_from = n,
              values_fill = list(n = 0)) %>%
  rename(EM2017 = `Emmanuel Macron (2017)`,
         EM2022 = `Emmanuel Macron (2022)`) %>%
  # Add `+1` to all values so that the frequency is greater than zero
  mutate(ratio_EM2017 = ((EM2017 + 1) / (sum(EM2017 + 1))), # Weight of words in EM2017 speech
         ratio_EM2022 = ((EM2022 + 1) / (sum(EM2022 + 1))), # Weight of words in EM2022 speech
         odds_ratio = ratio_EM2017 / ratio_EM2022,
         log_odds_ratio = log(odds_ratio)) %>%
  group_by(President = ifelse(log_odds_ratio > 0, "EM2017", "EM2022")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F) %>%
  select(word, log_odds_ratio, President)

# Refill president column with President's full name
top10lor$President[top10lor$President == "EM2017"] <- "Emmanuel Macron (2017)"
top10lor$President[top10lor$President == "EM2022"] <- "Emmanuel Macron (2022)"

# Historical order
top10lor$President <- factor(top10lor$President,
                             levels = c("Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create the top 10 log odds ratio bar graph
ggplot(top10lor, aes(x = reorder_within(word, log_odds_ratio, President), # To order by log odds ratio
                  y = log_odds_ratio,
                  fill = President)) +
  geom_col(show.legend = F) + # To remove President legend
  coord_flip() + # To flip axis
  facet_wrap(~President, scales = "free", ncol = 2) + # To remove unused top10 words + space on x axis
  scale_x_reordered() + # To remove x axis legends
  labs(x = NULL) + # To remove reorder labels
  ggtitle("Top 10 words by Log Odds Ration for Macron's speeches") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5), # Bold title, centered
        panel.grid.major.y = element_blank()) # Remove y-axis grid lines
```

- The *sign** and **magnitude** tell us which words are more important in the speech
- A positive number greater than 0 indicates greater importance in E.M. 2017's speech
- Negative numbers less than 0 have more weight in E.M. 2022's speech
- If it is close to 0, it has similar weight in both speeches
<br />
<br />

# Sentimental Analysis using 'bing' lexicon
Goal: Compare for each speech the distribution of positive and negative words.
Then, to compare only the positive and negative aspects, let's use "bing", which is the most inclusive lexicon for this.
```{r}
speeches_posneg <- speeches_tidy %>%
  inner_join(get_sentiments("bing")) %>% # Use 'bing' lexicon
  count(President, sentiment) %>%
  group_by(President) %>%
  mutate(percentage = n / sum(n) * 100) %>% # Compute distribution in %
  rename(Sentiment = sentiment) # Rename sentiment column for aesthetics reason (legend displaying)

# Historical order
speeches_posneg$President <- factor(speeches_posneg$President,
                          levels = c("Valéry Giscard d'Estaing (1974)", "François Mitterrand (1981)",	"François Mitterrand (1988)", "Jacques Chirac (1995)", "Jacques Chirac (2002)", "Nicolas Sarkozy (2007)", "François Hollande (2012)", "Emmanuel Macron (2017)", "Emmanuel Macron (2022)"))

# Create the circular graphs
ggplot(speeches_posneg, aes(x = "", y = percentage, fill = Sentiment)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  geom_text(aes(
    label = paste0(round(percentage), "%")), # Display percentage values on each graph
    position = position_stack(vjust = 0.5),
    color = "white",
    size = 3) +
  coord_polar("y", start = 0) +
  facet_wrap(~President, strip.position = "bottom", ncol = 3, labeller = label_wrap_gen(width = 20)) +
  scale_fill_manual(values = c("negative" = "red", "positive" = "darkgreen")) +
  theme_void() + # Hide ugly graph details
  ggtitle("Speeches positive/negative words distribution") +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold", hjust = 0.5),
    strip.text = element_text(size = 8, face = "bold"),
    legend.margin = margin(t = 10, r = 10, b = 10, l = 50)
  )
```

<br />

# Speeches' Clusters by analyzing consecutive word pairs with n-gram
By making a network graph with Bigrams, we can:

  - Focus on pairs of words that occur frequently together in succession
  - Represented by highly related and simultaneously frequent words
  - Nodes are mostly connected
  - Clustering of words is less clear, but overall relationship of words can be seen
```{r}
# Creating our Bigrams data frame
bigram_speeches <- speeches_data %>%
  unnest_tokens(input = Speech,
                output = bigram,
                token = "ngrams",
                n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  # Remove stop words
  filter(!word1 %in% c(english_stopwords, french_stopwords),
         !word2 %in% c(english_stopwords, french_stopwords)) %>%
  count(word1, word2, sort = T) %>%
  # Remove NA values after stop words cleaning
  na.omit()

# Creating a network graph data
graph_bigram <- bigram_speeches %>%
  # Flimsy filter (we don't have a lot of data here)
  filter(n >= 3) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(), # Compute centrality
         group = as.factor(group_infomap())) # Compute community

# Creating our Bigrams network graph
set.seed(1234)
ggraph(graph_bigram, layout = "fr") +
  geom_edge_link(color = "gray50",       # Edge color
                 alpha = 0.5) +          # Edge contrast
  geom_node_point(aes(size = centrality, # Node size
                      color = group),    # Node color
                  show.legend = F) +     # Legend removal
  scale_size(range = c(5, 10)) +         # Range of node size
  geom_node_text(aes(label = name),
                 repel = T,
                 size = 5) +
  theme_graph() +
  ggtitle("Speeches Bigrams network graph") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) # Bold title, centered
```

N-grams:

- Center around pairs of words that make sense when used together in succession
- When expressing the overall relationship between words  
<br />

# Speeches' Clusters with Phi Coefficient
By making a network graph with Phi Coefficient, we can:

- Network around highly relevant word pairs
- Highly relevant words are represented even if they are less frequent
- Less relevant nodes are not connected
- Clusters of words are clearly visible, but it is difficult to see the overall relationship between words
```{r}
# Creating our Phi Coefficient data frame
phi_speeches <- speeches_tidy %>%
  add_count(word) %>%
  # Flimsy filter (we don't have a lot of data here)
  filter(n >= 5) %>%
  pairwise_cor(item = word, # Compute Phi Coefficient
               feature = President, 
               sort = T)

# Creating a network graph data
graph_phi <- phi_speeches %>%
  # Correlation must be equal or bigger than 0.05
  filter(correlation >= 0.05) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(),
         group = as.factor(group_infomap()))

# Creating our Bigrams network graph
set.seed(1234)
ggraph(graph_phi, layout = "fr") +
  geom_edge_link(color = "gray50",
                 aes(edge_alpha = correlation, # Edge contrast
                     edge_width = 1),          # Edge thickness
                 show.legend = F) +            # Legend removal
  scale_edge_width(range = c(1, 2)) +          # Edge thickness removal
  geom_node_point(aes(size = centrality,
                      color = group),
                  show.legend = F) +
  scale_size(range = c(5, 10)) +
  geom_node_text(aes(label = name),
                 repel = T,
                 size = 5) +
  theme_graph() +
  ggtitle("Speeches Phi Coefficient network graph") +
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) # Bold title, centered
```
Phi coefficient:

- Focus on highly related word pairs
- When you want to highlight clusters of words